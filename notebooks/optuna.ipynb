{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import sqrtm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Dense, LSTM, Embedding, LeakyReLU, BatchNormalization, Dropout, Input, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale data between 0 and 1\n",
    "def scale_data(data):\n",
    "    min_val = data.min().min()  # Get the global minimum\n",
    "    max_val = data.max().max()  # Get the global maximum\n",
    "    scaled_data = (data - min_val) / (max_val - min_val)  # Apply min-max scaling\n",
    "    return scaled_data\n",
    "\n",
    "def build_generator(latent_dim, data_shape, num_layers=4, num_neurons=256, activation='relu', dropout=0.2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(num_neurons, input_dim=latent_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(num_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "\n",
    "    model.add(Dense(data_shape, activation='sigmoid'))\n",
    "    \n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    generated_data = model(noise)\n",
    "    \n",
    "    return Model(noise, generated_data)\n",
    "\n",
    "def build_discriminator(data_shape, num_layers=4, num_neurons=256, activation='relu', dropout=0.2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(num_neurons, input_dim=data_shape))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(num_neurons))\n",
    "        model.add(LeakyReLU())  \n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    data = Input(shape=(data_shape,))\n",
    "    validity = model(data)\n",
    "    \n",
    "    return Model(data, validity)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "def gradient_penalty(real_data, fake_data, discriminator):\n",
    "    alpha = tf.random.normal([real_data.shape[0], 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        prediction = discriminator(interpolated)\n",
    "    gradients = tape.gradient(prediction, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1]))\n",
    "    penalty = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return penalty\n",
    "\n",
    "def compute_gradient_norm(model, real_data, fake_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Obliczanie straty dyskryminatora\n",
    "        real_output = model(real_data, training=True)\n",
    "        fake_output = model(fake_data, training=True)\n",
    "        loss = wasserstein_loss(real_output, fake_output)\n",
    "\n",
    "    # Obliczanie gradientów względem wag dyskryminatora\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Obliczanie normy gradientów\n",
    "    gradient_norm = tf.linalg.global_norm(gradients)\n",
    "    return gradient_norm.numpy()\n",
    "\n",
    "def inverse_scale_data(generated_data, original_min, original_max):\n",
    "    # Przeskaluj dane z powrotem do oryginalnego zakresu\n",
    "    inversed_data = generated_data * (original_max - original_min) + original_min\n",
    "    return inversed_data\n",
    "\n",
    "def generate_data(generator, n_samples, latent_dim, original_min, original_max):\n",
    "    noise = np.random.normal(0, 1, size=(n_samples, latent_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "    generated_data = inverse_scale_data(generated_data, original_min, original_max)\n",
    "    return generated_data\n",
    "\n",
    "# FID score function\n",
    "def calculate_fid(real_data, generated_data):\n",
    "    # Przygotowanie danych\n",
    "    real_data = real_data.astype('float32')\n",
    "    generated_data = generated_data.astype('float32')\n",
    "    # Obliczanie średniej i macierzy kowariancji dla prawdziwych danych\n",
    "    mu1, sigma1 = real_data.mean(axis=0), np.cov(real_data, rowvar=False)\n",
    "    # Obliczanie średniej i macierzy kowariancji dla wygenerowanych danych\n",
    "    mu2, sigma2 = generated_data.mean(axis=0), np.cov(generated_data, rowvar=False)\n",
    "    # Obliczanie sumy kwadratów różnicy średnich\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # Obliczanie pierwiastka z iloczynu macierzy kowariancji\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # Sprawdzanie, czy macierz kowariancji jest poprawna\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # Obliczanie FID score\n",
    "    fid_score = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid_score\n",
    "\n",
    "def train(generator, discriminator, combined, data, latent_dim, epochs, batch_size=128, n_critic=5, lambda_gp=10.0, save_interval=50):\n",
    "    # Ładowanie i skalowanie danych\n",
    "    X_train = data.values\n",
    "\n",
    "    # Initialize lists to store loss values\n",
    "    d_loss_list = []\n",
    "    g_loss_list = []\n",
    "    gradient_norm_list = []\n",
    "    gradient_norm = 0\n",
    "    # Pętla po epokach\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for _ in range(n_critic):\n",
    "            # ---------------------\n",
    "            #  Trenowanie dyskryminatora\n",
    "            # ---------------------\n",
    "            \n",
    "            discriminator.trainable = True\n",
    "\n",
    "            # Wybieranie losowych próbek\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_data = X_train[idx]\n",
    "\n",
    "            # Generowanie nowego szumu\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "            # Generowanie nowych danych\n",
    "            generated_data = generator.predict(noise)\n",
    "\n",
    "            # Trenowanie dyskryminatora\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, -np.ones((batch_size, 1), dtype=np.float32))\n",
    "            d_loss_fake = discriminator.train_on_batch(generated_data, np.ones((batch_size, 1), dtype=np.float32))\n",
    "            d_loss = d_loss_fake - d_loss_real + lambda_gp * gradient_penalty(real_data, generated_data, discriminator)\n",
    "\n",
    "            gradient_norm = compute_gradient_norm(discriminator, real_data, generated_data)\n",
    "            gradient_norm_list.append(gradient_norm)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Trenowanie generatora\n",
    "        # ---------------------\n",
    "\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "        # Chcemy, aby dyskryminator uznał wygenerowane dane za prawdziwe\n",
    "        valid_y = np.array([1] * batch_size, dtype=np.float32)\n",
    "\n",
    "        # Trenowanie generatora\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "        # Append loss values to lists\n",
    "        d_loss_list.append(d_loss)\n",
    "        g_loss_list.append(g_loss)\n",
    "\n",
    "        # Zapisywanie postępów\n",
    "        if epoch % save_interval == 0:\n",
    "            print(\"%d [D loss: %f] [G loss: %f] [D gradient norm: %f]\" % (epoch, d_loss, g_loss, gradient_norm))\n",
    "            print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "            #checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    return d_loss_list, g_loss_list, gradient_norm_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = 'original_data/GSE158508_normalized_counts.tsv'\n",
    "data = pd.read_csv(data_path, sep='\\t', index_col=0)\n",
    "\n",
    "data_shape = data.shape[1]  # 69 columns\n",
    "\n",
    "# Save columns names for later use\n",
    "col_names = data.columns.values\n",
    "\n",
    "original_min = data.min().min()\n",
    "original_max = data.max().max()\n",
    "\n",
    "original_data = data\n",
    "data = scale_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Sugerowanie hiperparametrów\n",
    "    num_layers_generator = trial.suggest_int('num_layers_generator', 2, 6)\n",
    "    num_layers_discriminator = trial.suggest_int('num_layers_discriminator', 2, 6)\n",
    "    neurons_per_layer_generator = trial.suggest_categorical('neurons_per_layer_generator', [64, 128, 256, 512])\n",
    "    neurons_per_layer_discriminator = trial.suggest_categorical('neurons_per_layer_discriminator', [64, 128, 256, 512])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True)\n",
    "    n_critic = trial.suggest_int('n_critic', 1, 5)\n",
    "    lambda_gp = trial.suggest_float('lambda_gp', 1e-2, 1e2, log=True)\n",
    "    latent_dim = trial.suggest_int('latent_dim', 32, 128)\n",
    "    batch_size = 32\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['RMSprop', 'Adam'])\n",
    "    if optimizer == 'RMSprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    discriminator = build_discriminator(data_shape, num_layers=num_layers_discriminator, num_neurons=neurons_per_layer_discriminator)\n",
    "    discriminator.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    generator = build_generator(latent_dim, data_shape, num_layers=num_layers_generator, num_neurons=neurons_per_layer_generator)\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    generated_data = generator(z)\n",
    "    discriminator.trainable = False\n",
    "    validity = discriminator(generated_data)\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    train(generator, discriminator, combined, data, latent_dim, epochs=5000, batch_size=batch_size, n_critic=n_critic, lambda_gp=lambda_gp, save_interval=100)\n",
    "\n",
    "    synthetic_data = generate_data(generator, n_samples=57736, latent_dim=latent_dim, original_min=original_min, original_max=original_max)\n",
    "    df = pd.DataFrame(synthetic_data)\n",
    "    df.columns = col_names\n",
    "    df.to_csv('synthetic_data/generated_data.tsv', sep='\\t', index=False, header=True)\n",
    "    df = pd.read_csv('synthetic_data/generated_data.tsv', sep='\\t')\n",
    "    synthetic_data = df\n",
    "\n",
    "    # Obliczanie metryki, która ma być optymalizowana\n",
    "    fid_score = calculate_fid(original_data, synthetic_data)\n",
    "\n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-11 23:34:18,327] A new study created in memory with name: no-name-3335bdbf-c18b-4af6-8615-24f448070d07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 41.106987] [G loss: 0.484376] [D gradient norm: 0.575881]\n",
      "Time for epoch 1 is 1.774334192276001 sec\n",
      "100 [D loss: 41.204723] [G loss: 0.005452] [D gradient norm: 0.061868]\n",
      "Time for epoch 101 is 0.37404513359069824 sec\n",
      "200 [D loss: 48.340004] [G loss: 0.001774] [D gradient norm: 0.184417]\n",
      "Time for epoch 201 is 0.4197566509246826 sec\n",
      "300 [D loss: 83.167107] [G loss: 0.000307] [D gradient norm: 0.031936]\n",
      "Time for epoch 301 is 0.3801412582397461 sec\n",
      "400 [D loss: 67.965309] [G loss: 0.000411] [D gradient norm: 0.040873]\n",
      "Time for epoch 401 is 0.32248687744140625 sec\n",
      "500 [D loss: 60.609570] [G loss: 0.000042] [D gradient norm: 0.004021]\n",
      "Time for epoch 501 is 0.33448171615600586 sec\n",
      "600 [D loss: 64.912117] [G loss: 0.000035] [D gradient norm: 0.008058]\n",
      "Time for epoch 601 is 0.6025094985961914 sec\n",
      "700 [D loss: 61.974445] [G loss: 0.000024] [D gradient norm: 0.001747]\n",
      "Time for epoch 701 is 0.47564244270324707 sec\n",
      "800 [D loss: 70.976601] [G loss: 0.000017] [D gradient norm: 0.000823]\n",
      "Time for epoch 801 is 0.33927178382873535 sec\n",
      "900 [D loss: 81.789566] [G loss: 0.000008] [D gradient norm: 0.000593]\n",
      "Time for epoch 901 is 0.36666154861450195 sec\n",
      "1000 [D loss: 83.440727] [G loss: 0.000007] [D gradient norm: 0.000400]\n",
      "Time for epoch 1001 is 0.38126206398010254 sec\n",
      "1100 [D loss: 78.879425] [G loss: 0.000006] [D gradient norm: 0.000382]\n",
      "Time for epoch 1101 is 0.30600476264953613 sec\n",
      "1200 [D loss: 46.378956] [G loss: 0.000003] [D gradient norm: 0.000397]\n",
      "Time for epoch 1201 is 0.37003016471862793 sec\n",
      "1300 [D loss: 126.493935] [G loss: 0.000002] [D gradient norm: 0.000206]\n",
      "Time for epoch 1301 is 0.3219265937805176 sec\n",
      "1400 [D loss: 149.843430] [G loss: 0.000001] [D gradient norm: 0.000084]\n",
      "Time for epoch 1401 is 0.3429107666015625 sec\n",
      "1500 [D loss: 114.845810] [G loss: 0.000001] [D gradient norm: 0.000086]\n",
      "Time for epoch 1501 is 0.4148879051208496 sec\n",
      "1600 [D loss: 46.148136] [G loss: 0.000001] [D gradient norm: 0.000234]\n",
      "Time for epoch 1601 is 0.4322628974914551 sec\n",
      "1700 [D loss: 44.740437] [G loss: 0.000001] [D gradient norm: 0.000083]\n",
      "Time for epoch 1701 is 0.49526238441467285 sec\n",
      "1800 [D loss: 196.096832] [G loss: 0.000001] [D gradient norm: 0.000093]\n",
      "Time for epoch 1801 is 0.4973599910736084 sec\n",
      "1900 [D loss: 47.165127] [G loss: 0.000001] [D gradient norm: 0.000075]\n",
      "Time for epoch 1901 is 0.5318968296051025 sec\n",
      "2000 [D loss: 112.434898] [G loss: 0.000000] [D gradient norm: 0.000011]\n",
      "Time for epoch 2001 is 0.3926808834075928 sec\n",
      "2100 [D loss: 44.041603] [G loss: 0.000000] [D gradient norm: 0.000008]\n",
      "Time for epoch 2101 is 0.5550134181976318 sec\n",
      "2200 [D loss: 48.355869] [G loss: 0.000000] [D gradient norm: 0.000025]\n",
      "Time for epoch 2201 is 0.4035816192626953 sec\n",
      "2300 [D loss: 123.923180] [G loss: 0.000000] [D gradient norm: 0.000011]\n",
      "Time for epoch 2301 is 0.4279792308807373 sec\n",
      "2400 [D loss: 47.726494] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 2401 is 0.5456559658050537 sec\n",
      "2500 [D loss: 48.538326] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 2501 is 0.5410017967224121 sec\n",
      "2600 [D loss: 174.017899] [G loss: 0.000000] [D gradient norm: 0.000006]\n",
      "Time for epoch 2601 is 0.355165958404541 sec\n",
      "2700 [D loss: 137.524918] [G loss: 0.000000] [D gradient norm: 0.000007]\n",
      "Time for epoch 2701 is 0.36861085891723633 sec\n",
      "2800 [D loss: 45.740662] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 2801 is 0.319608211517334 sec\n",
      "2900 [D loss: 46.780170] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 2901 is 0.5063719749450684 sec\n",
      "3000 [D loss: 46.656460] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3001 is 0.5085465908050537 sec\n",
      "3100 [D loss: 46.591946] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3101 is 0.5783982276916504 sec\n",
      "3200 [D loss: 252.989807] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3201 is 0.5497581958770752 sec\n",
      "3300 [D loss: 129.656525] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3301 is 0.49051451683044434 sec\n",
      "3400 [D loss: 103.877235] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3401 is 0.4875338077545166 sec\n",
      "3500 [D loss: 44.102448] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.4930424690246582 sec\n",
      "3600 [D loss: 46.314651] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3601 is 0.5468153953552246 sec\n",
      "3700 [D loss: 210.834900] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.5320143699645996 sec\n",
      "3800 [D loss: 70.226517] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3801 is 0.51055908203125 sec\n",
      "3900 [D loss: 162.507477] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3901 is 0.5199074745178223 sec\n",
      "4000 [D loss: 106.376564] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4001 is 0.5391888618469238 sec\n",
      "4100 [D loss: 48.287052] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.5211520195007324 sec\n",
      "4200 [D loss: 81.390259] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.5308072566986084 sec\n",
      "4300 [D loss: 79.169029] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 0.5122721195220947 sec\n",
      "4400 [D loss: 49.767723] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.5173206329345703 sec\n",
      "4500 [D loss: 48.269615] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4501 is 0.5444207191467285 sec\n",
      "4600 [D loss: 299.392273] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.5338830947875977 sec\n",
      "4700 [D loss: 241.972031] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4701 is 0.5242369174957275 sec\n",
      "4800 [D loss: 65.797585] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4801 is 0.5204377174377441 sec\n",
      "4900 [D loss: 48.318611] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.5155158042907715 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 00:12:24,015] Trial 0 finished with value: 5630.636550454829 and parameters: {'num_layers_generator': 5, 'num_layers_discriminator': 4, 'neurons_per_layer_generator': 256, 'neurons_per_layer_discriminator': 128, 'learning_rate': 2.857417833645217e-05, 'n_critic': 3, 'lambda_gp': 47.36625865879427, 'latent_dim': 57, 'optimizer': 'Adam'}. Best is trial 0 with value: 5630.636550454829.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.649162] [G loss: 0.490740] [D gradient norm: 0.528372]\n",
      "Time for epoch 1 is 3.362375259399414 sec\n",
      "100 [D loss: 1.175102] [G loss: 0.008779] [D gradient norm: 0.062632]\n",
      "Time for epoch 101 is 0.7754716873168945 sec\n",
      "200 [D loss: 1.413219] [G loss: 0.003679] [D gradient norm: 0.099189]\n",
      "Time for epoch 201 is 0.7162075042724609 sec\n",
      "300 [D loss: 1.622063] [G loss: 0.001381] [D gradient norm: 0.229910]\n",
      "Time for epoch 301 is 0.7513277530670166 sec\n",
      "400 [D loss: 2.300693] [G loss: 0.000444] [D gradient norm: 0.053428]\n",
      "Time for epoch 401 is 0.715470552444458 sec\n",
      "500 [D loss: 2.763347] [G loss: 0.000009] [D gradient norm: 0.003511]\n",
      "Time for epoch 501 is 0.706599235534668 sec\n",
      "600 [D loss: 2.229146] [G loss: 0.000001] [D gradient norm: 0.002277]\n",
      "Time for epoch 601 is 0.744534969329834 sec\n",
      "700 [D loss: 3.323611] [G loss: 0.000000] [D gradient norm: 0.000252]\n",
      "Time for epoch 701 is 0.7530486583709717 sec\n",
      "800 [D loss: 1.793557] [G loss: 0.000000] [D gradient norm: 0.000012]\n",
      "Time for epoch 801 is 0.7380261421203613 sec\n",
      "900 [D loss: 2.536080] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 901 is 0.7148020267486572 sec\n",
      "1000 [D loss: 4.013065] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1001 is 0.7047939300537109 sec\n",
      "1100 [D loss: 1.751681] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 1101 is 0.724301815032959 sec\n",
      "1200 [D loss: 3.277201] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1201 is 0.7467489242553711 sec\n",
      "1300 [D loss: 1.727840] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 1301 is 0.7135143280029297 sec\n",
      "1400 [D loss: 1.751569] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1401 is 0.7714841365814209 sec\n",
      "1500 [D loss: 3.567959] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1501 is 0.725147008895874 sec\n",
      "1600 [D loss: 1.745818] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1601 is 0.7397201061248779 sec\n",
      "1700 [D loss: 2.018527] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1701 is 0.7304432392120361 sec\n",
      "1800 [D loss: 3.089406] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1801 is 0.7510621547698975 sec\n",
      "1900 [D loss: 4.386964] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 1901 is 0.7466115951538086 sec\n",
      "2000 [D loss: 7.368024] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2001 is 0.7463009357452393 sec\n",
      "2100 [D loss: 1.711427] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2101 is 0.7534866333007812 sec\n",
      "2200 [D loss: 1.748282] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2201 is 0.7276146411895752 sec\n",
      "2300 [D loss: 5.563645] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2301 is 0.7142679691314697 sec\n",
      "2400 [D loss: 1.723912] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2401 is 0.766594409942627 sec\n",
      "2500 [D loss: 2.815847] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2501 is 0.6879823207855225 sec\n",
      "2600 [D loss: 7.661755] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 2601 is 0.7489557266235352 sec\n",
      "2700 [D loss: 1.728711] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2701 is 0.7140243053436279 sec\n",
      "2800 [D loss: 2.946611] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2801 is 0.7513813972473145 sec\n",
      "2900 [D loss: 5.077115] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2901 is 0.8024449348449707 sec\n",
      "3000 [D loss: 1.748126] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3001 is 0.7359485626220703 sec\n",
      "3100 [D loss: 2.162663] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3101 is 0.7179172039031982 sec\n",
      "3200 [D loss: 3.682076] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3201 is 0.7227535247802734 sec\n",
      "3300 [D loss: 1.753745] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3301 is 0.7330002784729004 sec\n",
      "3400 [D loss: 10.800230] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3401 is 0.7350876331329346 sec\n",
      "3500 [D loss: 3.602217] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.6957366466522217 sec\n",
      "3600 [D loss: 2.299503] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3601 is 0.7154383659362793 sec\n",
      "3700 [D loss: 5.602307] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.7182934284210205 sec\n",
      "3800 [D loss: 1.743905] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3801 is 0.7150218486785889 sec\n",
      "3900 [D loss: 1.729755] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3901 is 0.7200777530670166 sec\n",
      "4000 [D loss: 6.210902] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4001 is 0.7000443935394287 sec\n",
      "4100 [D loss: 1.745875] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.743384599685669 sec\n",
      "4200 [D loss: 1.736489] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.7169525623321533 sec\n",
      "4300 [D loss: 2.886034] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 0.7054665088653564 sec\n",
      "4400 [D loss: 6.746372] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.7801690101623535 sec\n",
      "4500 [D loss: 5.984843] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4501 is 0.7513411045074463 sec\n",
      "4600 [D loss: 1.778720] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.7469401359558105 sec\n",
      "4700 [D loss: 1.753403] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4701 is 0.7898778915405273 sec\n",
      "4800 [D loss: 1.735694] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4801 is 0.7512383460998535 sec\n",
      "4900 [D loss: 1.815686] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.772545576095581 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 01:14:04,596] Trial 1 finished with value: 5424.101562877995 and parameters: {'num_layers_generator': 4, 'num_layers_discriminator': 5, 'neurons_per_layer_generator': 256, 'neurons_per_layer_discriminator': 128, 'learning_rate': 1.3220810900803522e-05, 'n_critic': 4, 'lambda_gp': 0.7537767976931569, 'latent_dim': 125, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 5424.101562877995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 17.720921] [G loss: 0.477771] [D gradient norm: 0.551153]\n",
      "Time for epoch 1 is 4.460965633392334 sec\n",
      "100 [D loss: 16.784433] [G loss: 0.006530] [D gradient norm: 0.047665]\n",
      "Time for epoch 101 is 0.7872233390808105 sec\n",
      "200 [D loss: 14.739384] [G loss: 0.004465] [D gradient norm: 0.136508]\n",
      "Time for epoch 201 is 0.8154234886169434 sec\n",
      "300 [D loss: 35.367416] [G loss: 0.000839] [D gradient norm: 0.104046]\n",
      "Time for epoch 301 is 0.822791576385498 sec\n",
      "400 [D loss: 45.060390] [G loss: 0.000195] [D gradient norm: 0.646093]\n",
      "Time for epoch 401 is 0.7730007171630859 sec\n",
      "500 [D loss: 45.066650] [G loss: 0.000265] [D gradient norm: 0.018165]\n",
      "Time for epoch 501 is 0.813380241394043 sec\n",
      "600 [D loss: 108.132027] [G loss: 0.000274] [D gradient norm: 0.043598]\n",
      "Time for epoch 601 is 0.7907416820526123 sec\n",
      "700 [D loss: 18.292007] [G loss: 0.000131] [D gradient norm: 0.025644]\n",
      "Time for epoch 701 is 0.8513355255126953 sec\n",
      "800 [D loss: 17.074478] [G loss: 0.000019] [D gradient norm: 0.019008]\n",
      "Time for epoch 801 is 0.8277847766876221 sec\n",
      "900 [D loss: 157.774567] [G loss: 0.000070] [D gradient norm: 0.051031]\n",
      "Time for epoch 901 is 0.7797756195068359 sec\n",
      "1000 [D loss: 17.606453] [G loss: 0.001177] [D gradient norm: 0.002423]\n",
      "Time for epoch 1001 is 0.8051393032073975 sec\n",
      "1100 [D loss: 120.476410] [G loss: 0.000067] [D gradient norm: 0.002699]\n",
      "Time for epoch 1101 is 0.7395968437194824 sec\n",
      "1200 [D loss: 106.425743] [G loss: 0.000013] [D gradient norm: 0.000784]\n",
      "Time for epoch 1201 is 0.7885146141052246 sec\n",
      "1300 [D loss: 18.120893] [G loss: 0.000001] [D gradient norm: 0.001171]\n",
      "Time for epoch 1301 is 0.744948148727417 sec\n",
      "1400 [D loss: 18.427635] [G loss: 0.000000] [D gradient norm: 0.000243]\n",
      "Time for epoch 1401 is 0.7812228202819824 sec\n",
      "1500 [D loss: 18.882833] [G loss: 0.000004] [D gradient norm: 0.000070]\n",
      "Time for epoch 1501 is 0.7487304210662842 sec\n",
      "1600 [D loss: 66.851212] [G loss: 0.000009] [D gradient norm: 0.000229]\n",
      "Time for epoch 1601 is 0.7812294960021973 sec\n",
      "1700 [D loss: 204.337997] [G loss: 0.000000] [D gradient norm: 0.000427]\n",
      "Time for epoch 1701 is 0.7836754322052002 sec\n",
      "1800 [D loss: 29.201622] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 1801 is 0.7840080261230469 sec\n",
      "1900 [D loss: 17.520491] [G loss: 0.000009] [D gradient norm: 0.000011]\n",
      "Time for epoch 1901 is 0.7595353126525879 sec\n",
      "2000 [D loss: 29.852955] [G loss: 0.000000] [D gradient norm: 0.000662]\n",
      "Time for epoch 2001 is 0.7868702411651611 sec\n",
      "2100 [D loss: 245.266510] [G loss: 0.000000] [D gradient norm: 0.000007]\n",
      "Time for epoch 2101 is 0.7992007732391357 sec\n",
      "2200 [D loss: 18.388540] [G loss: 0.000008] [D gradient norm: 0.000020]\n",
      "Time for epoch 2201 is 0.8860437870025635 sec\n",
      "2300 [D loss: 225.042099] [G loss: 0.000001] [D gradient norm: 0.000048]\n",
      "Time for epoch 2301 is 0.8227736949920654 sec\n",
      "2400 [D loss: 144.264633] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 2401 is 0.8367812633514404 sec\n",
      "2500 [D loss: 18.180445] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 2501 is 0.8196539878845215 sec\n",
      "2600 [D loss: 18.593115] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 2601 is 0.7955701351165771 sec\n",
      "2700 [D loss: 41.877224] [G loss: 0.000000] [D gradient norm: 0.000008]\n",
      "Time for epoch 2701 is 0.8597133159637451 sec\n",
      "2800 [D loss: 18.705946] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2801 is 0.8164303302764893 sec\n",
      "2900 [D loss: 18.515846] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 2901 is 0.805527925491333 sec\n",
      "3000 [D loss: 115.169601] [G loss: 0.000000] [D gradient norm: 0.000075]\n",
      "Time for epoch 3001 is 0.7705121040344238 sec\n",
      "3100 [D loss: 24.140059] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 3101 is 0.7735860347747803 sec\n",
      "3200 [D loss: 29.116259] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3201 is 0.8272323608398438 sec\n",
      "3300 [D loss: 353.162628] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 3301 is 0.7667174339294434 sec\n",
      "3400 [D loss: 106.206963] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3401 is 0.7536368370056152 sec\n",
      "3500 [D loss: 17.816751] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.8044817447662354 sec\n",
      "3600 [D loss: 246.857513] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3601 is 0.7837619781494141 sec\n",
      "3700 [D loss: 121.293236] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.7592668533325195 sec\n",
      "3800 [D loss: 159.137192] [G loss: 0.000000] [D gradient norm: 0.000009]\n",
      "Time for epoch 3801 is 0.8038938045501709 sec\n",
      "3900 [D loss: 322.837952] [G loss: 0.000000] [D gradient norm: 0.000052]\n",
      "Time for epoch 3901 is 0.8180992603302002 sec\n",
      "4000 [D loss: 206.263290] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4001 is 0.8355295658111572 sec\n",
      "4100 [D loss: 239.237137] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.7617788314819336 sec\n",
      "4200 [D loss: 18.500313] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.8244936466217041 sec\n",
      "4300 [D loss: 18.926573] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 4301 is 0.8329112529754639 sec\n",
      "4400 [D loss: 430.980682] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 4401 is 0.8143279552459717 sec\n",
      "4500 [D loss: 90.258987] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 4501 is 0.7527971267700195 sec\n",
      "4600 [D loss: 312.230927] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.8341410160064697 sec\n",
      "4700 [D loss: 520.605957] [G loss: 0.000000] [D gradient norm: 0.000070]\n",
      "Time for epoch 4701 is 0.7903902530670166 sec\n",
      "4800 [D loss: 20.765976] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 4801 is 0.807030200958252 sec\n",
      "4900 [D loss: 201.144379] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.7944965362548828 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 02:20:30,814] Trial 2 finished with value: 4432.503600472038 and parameters: {'num_layers_generator': 6, 'num_layers_discriminator': 6, 'neurons_per_layer_generator': 64, 'neurons_per_layer_discriminator': 256, 'learning_rate': 6.788701065963404e-06, 'n_critic': 4, 'lambda_gp': 17.990064999862028, 'latent_dim': 37, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 4432.503600472038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.342915] [G loss: 0.457533] [D gradient norm: 0.729386]\n",
      "Time for epoch 1 is 4.039901494979858 sec\n",
      "100 [D loss: 1.126391] [G loss: 0.002258] [D gradient norm: 0.095714]\n",
      "Time for epoch 101 is 0.8937122821807861 sec\n",
      "200 [D loss: 1.457944] [G loss: 0.000030] [D gradient norm: 0.068008]\n",
      "Time for epoch 201 is 0.893413782119751 sec\n",
      "300 [D loss: 1.930773] [G loss: 0.000000] [D gradient norm: 0.000677]\n",
      "Time for epoch 301 is 0.9376592636108398 sec\n",
      "400 [D loss: 2.288710] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 401 is 0.9176197052001953 sec\n",
      "500 [D loss: 1.455430] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 501 is 0.9051640033721924 sec\n",
      "600 [D loss: 2.936726] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 601 is 0.9091641902923584 sec\n",
      "700 [D loss: 1.444124] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 701 is 0.9124810695648193 sec\n",
      "800 [D loss: 2.420095] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 801 is 0.9389317035675049 sec\n",
      "900 [D loss: 2.370686] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 901 is 0.9757211208343506 sec\n",
      "1000 [D loss: 2.005907] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1001 is 0.8996562957763672 sec\n",
      "1100 [D loss: 2.419142] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1101 is 0.90488600730896 sec\n",
      "1200 [D loss: 2.498682] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1201 is 0.8696608543395996 sec\n",
      "1300 [D loss: 1.446964] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1301 is 0.9885125160217285 sec\n",
      "1400 [D loss: 2.263829] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1401 is 0.9574222564697266 sec\n",
      "1500 [D loss: 1.460245] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1501 is 0.9555668830871582 sec\n",
      "1600 [D loss: 3.996150] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1601 is 0.8810427188873291 sec\n",
      "1700 [D loss: 3.119689] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1701 is 0.88515305519104 sec\n",
      "1800 [D loss: 1.681604] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1801 is 0.8947930335998535 sec\n",
      "1900 [D loss: 1.443723] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1901 is 0.9026455879211426 sec\n",
      "2000 [D loss: 1.453953] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2001 is 0.9691414833068848 sec\n",
      "2100 [D loss: 1.564121] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2101 is 0.9162745475769043 sec\n",
      "2200 [D loss: 2.062957] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2201 is 0.8892359733581543 sec\n",
      "2300 [D loss: 1.459153] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2301 is 0.8701930046081543 sec\n",
      "2400 [D loss: 3.482302] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2401 is 0.9700698852539062 sec\n",
      "2500 [D loss: 1.450755] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2501 is 0.8751728534698486 sec\n",
      "2600 [D loss: 3.047818] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2601 is 0.9395711421966553 sec\n",
      "2700 [D loss: 1.450296] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2701 is 0.858952522277832 sec\n",
      "2800 [D loss: 3.523811] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2801 is 0.960956335067749 sec\n",
      "2900 [D loss: 1.821836] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2901 is 0.9155275821685791 sec\n",
      "3000 [D loss: 2.028431] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3001 is 0.8984925746917725 sec\n",
      "3100 [D loss: 1.441915] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3101 is 0.9175732135772705 sec\n",
      "3200 [D loss: 2.029931] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3201 is 0.9207611083984375 sec\n",
      "3300 [D loss: 1.448511] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3301 is 0.9036283493041992 sec\n",
      "3400 [D loss: 1.714741] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3401 is 0.9069488048553467 sec\n",
      "3500 [D loss: 2.080948] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.8902475833892822 sec\n",
      "3600 [D loss: 1.902559] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3601 is 0.9182088375091553 sec\n",
      "3700 [D loss: 1.441299] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.9029288291931152 sec\n",
      "3800 [D loss: 1.451557] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3801 is 0.9153058528900146 sec\n",
      "3900 [D loss: 3.863410] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3901 is 0.8919730186462402 sec\n",
      "4000 [D loss: 1.621899] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4001 is 0.9504725933074951 sec\n",
      "4100 [D loss: 2.358941] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.9422063827514648 sec\n",
      "4200 [D loss: 2.251041] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.876143217086792 sec\n",
      "4300 [D loss: 1.444892] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 0.8819379806518555 sec\n",
      "4400 [D loss: 1.759688] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.9096338748931885 sec\n",
      "4500 [D loss: 1.456068] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4501 is 0.858964204788208 sec\n",
      "4600 [D loss: 1.439079] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.8744945526123047 sec\n",
      "4700 [D loss: 1.458068] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4701 is 0.8679063320159912 sec\n",
      "4800 [D loss: 1.450376] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4801 is 0.8990786075592041 sec\n",
      "4900 [D loss: 2.770831] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.8986256122589111 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 03:36:13,762] Trial 3 finished with value: 5749.778984008037 and parameters: {'num_layers_generator': 5, 'num_layers_discriminator': 4, 'neurons_per_layer_generator': 512, 'neurons_per_layer_discriminator': 128, 'learning_rate': 2.4656336335976698e-05, 'n_critic': 5, 'lambda_gp': 0.4623301439076032, 'latent_dim': 71, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 4432.503600472038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.115644] [G loss: 0.511910] [D gradient norm: 0.572366]\n",
      "Time for epoch 1 is 2.972886800765991 sec\n",
      "100 [D loss: 1.075586] [G loss: 0.001044] [D gradient norm: 0.105033]\n",
      "Time for epoch 101 is 0.8495147228240967 sec\n",
      "200 [D loss: 1.114898] [G loss: 0.000001] [D gradient norm: 0.000156]\n",
      "Time for epoch 201 is 0.910092830657959 sec\n",
      "300 [D loss: 1.311929] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 301 is 0.8766181468963623 sec\n",
      "400 [D loss: 1.341224] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 401 is 0.9053623676300049 sec\n",
      "500 [D loss: 1.115117] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 501 is 0.8671789169311523 sec\n",
      "600 [D loss: 1.114683] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 601 is 0.8451848030090332 sec\n",
      "700 [D loss: 1.114214] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 701 is 0.8590936660766602 sec\n",
      "800 [D loss: 1.208825] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 801 is 0.8637819290161133 sec\n",
      "900 [D loss: 1.117372] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 901 is 0.8399937152862549 sec\n",
      "1000 [D loss: 1.113001] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1001 is 0.8751988410949707 sec\n",
      "1100 [D loss: 1.111232] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1101 is 0.8698663711547852 sec\n",
      "1200 [D loss: 1.118192] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1201 is 0.8764667510986328 sec\n",
      "1300 [D loss: 1.182577] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1301 is 0.8527851104736328 sec\n",
      "1400 [D loss: 1.815455] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1401 is 0.8785922527313232 sec\n",
      "1500 [D loss: 1.113092] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1501 is 0.8430085182189941 sec\n",
      "1600 [D loss: 1.570777] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1601 is 0.8584096431732178 sec\n",
      "1700 [D loss: 1.604305] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1701 is 0.8291661739349365 sec\n",
      "1800 [D loss: 1.114402] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1801 is 0.8755567073822021 sec\n",
      "1900 [D loss: 1.271003] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1901 is 0.9335527420043945 sec\n",
      "2000 [D loss: 1.292925] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2001 is 0.9762442111968994 sec\n",
      "2100 [D loss: 1.354513] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2101 is 0.9445478916168213 sec\n",
      "2200 [D loss: 1.462249] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2201 is 0.9598057270050049 sec\n",
      "2300 [D loss: 1.108350] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2301 is 0.9509742259979248 sec\n",
      "2400 [D loss: 1.118171] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2401 is 0.9111952781677246 sec\n",
      "2500 [D loss: 1.112191] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2501 is 0.9303727149963379 sec\n",
      "2600 [D loss: 1.117077] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2601 is 0.9141936302185059 sec\n",
      "2700 [D loss: 1.262111] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2701 is 0.9401099681854248 sec\n",
      "2800 [D loss: 1.283283] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2801 is 0.9147045612335205 sec\n",
      "2900 [D loss: 1.830175] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2901 is 0.9462776184082031 sec\n",
      "3000 [D loss: 1.186036] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3001 is 0.9344940185546875 sec\n",
      "3100 [D loss: 1.138186] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3101 is 0.9027111530303955 sec\n",
      "3200 [D loss: 1.116675] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3201 is 0.9131145477294922 sec\n",
      "3300 [D loss: 1.272065] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3301 is 0.895653486251831 sec\n",
      "3400 [D loss: 2.083842] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3401 is 0.8921098709106445 sec\n",
      "3500 [D loss: 1.565496] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.9231045246124268 sec\n",
      "3600 [D loss: 1.599122] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3601 is 0.8974306583404541 sec\n",
      "3700 [D loss: 1.359160] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.8699085712432861 sec\n",
      "3800 [D loss: 1.149990] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3801 is 0.9356467723846436 sec\n",
      "3900 [D loss: 1.142510] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3901 is 0.928966760635376 sec\n",
      "4000 [D loss: 1.115945] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4001 is 0.9353837966918945 sec\n",
      "4100 [D loss: 1.117742] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.9344925880432129 sec\n",
      "4200 [D loss: 1.612047] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.9378166198730469 sec\n",
      "4300 [D loss: 1.116350] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 0.8801395893096924 sec\n",
      "4400 [D loss: 1.116541] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.9272270202636719 sec\n",
      "4500 [D loss: 1.118245] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4501 is 0.9227769374847412 sec\n",
      "4600 [D loss: 1.115833] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.8963508605957031 sec\n",
      "4700 [D loss: 1.682793] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4701 is 0.9067203998565674 sec\n",
      "4800 [D loss: 1.122606] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4801 is 0.9252212047576904 sec\n",
      "4900 [D loss: 1.118277] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.907503604888916 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 04:52:20,357] Trial 4 finished with value: 5082.640447265859 and parameters: {'num_layers_generator': 2, 'num_layers_discriminator': 4, 'neurons_per_layer_generator': 256, 'neurons_per_layer_discriminator': 64, 'learning_rate': 7.799265308458153e-05, 'n_critic': 5, 'lambda_gp': 0.11827989236863701, 'latent_dim': 87, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 4432.503600472038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.059311] [G loss: 0.482665] [D gradient norm: 0.704354]\n",
      "Time for epoch 1 is 4.8736231327056885 sec\n",
      "100 [D loss: 0.585505] [G loss: 0.007996] [D gradient norm: 0.051406]\n",
      "Time for epoch 101 is 1.0376651287078857 sec\n",
      "200 [D loss: 0.683040] [G loss: 0.004852] [D gradient norm: 0.049125]\n",
      "Time for epoch 201 is 1.0257046222686768 sec\n",
      "300 [D loss: 0.903930] [G loss: 0.005455] [D gradient norm: 0.180120]\n",
      "Time for epoch 301 is 1.0030450820922852 sec\n",
      "400 [D loss: 1.073139] [G loss: 0.003463] [D gradient norm: 0.134093]\n",
      "Time for epoch 401 is 0.9907569885253906 sec\n",
      "500 [D loss: 1.079437] [G loss: 0.002557] [D gradient norm: 0.162540]\n",
      "Time for epoch 501 is 1.0239410400390625 sec\n",
      "600 [D loss: 1.188582] [G loss: 0.000484] [D gradient norm: 0.046909]\n",
      "Time for epoch 601 is 1.0590078830718994 sec\n",
      "700 [D loss: 1.141426] [G loss: 0.000156] [D gradient norm: 0.127763]\n",
      "Time for epoch 701 is 1.035064697265625 sec\n",
      "800 [D loss: 1.162729] [G loss: 0.000371] [D gradient norm: 0.018023]\n",
      "Time for epoch 801 is 0.9869194030761719 sec\n",
      "900 [D loss: 1.165295] [G loss: 0.000022] [D gradient norm: 0.008414]\n",
      "Time for epoch 901 is 1.0591402053833008 sec\n",
      "1000 [D loss: 1.091415] [G loss: 0.000051] [D gradient norm: 0.021326]\n",
      "Time for epoch 1001 is 0.9951245784759521 sec\n",
      "1100 [D loss: 1.127412] [G loss: 0.002469] [D gradient norm: 0.000355]\n",
      "Time for epoch 1101 is 1.0117073059082031 sec\n",
      "1200 [D loss: 1.264012] [G loss: 0.000000] [D gradient norm: 0.001945]\n",
      "Time for epoch 1201 is 0.9947454929351807 sec\n",
      "1300 [D loss: 1.567148] [G loss: 0.000126] [D gradient norm: 0.000080]\n",
      "Time for epoch 1301 is 1.0703136920928955 sec\n",
      "1400 [D loss: 1.169716] [G loss: 0.000002] [D gradient norm: 0.000644]\n",
      "Time for epoch 1401 is 1.0561928749084473 sec\n",
      "1500 [D loss: 1.649994] [G loss: 0.000000] [D gradient norm: 0.000012]\n",
      "Time for epoch 1501 is 1.0303540229797363 sec\n",
      "1600 [D loss: 1.080543] [G loss: 0.000000] [D gradient norm: 0.000050]\n",
      "Time for epoch 1601 is 0.9980897903442383 sec\n",
      "1700 [D loss: 1.675870] [G loss: 0.000000] [D gradient norm: 0.000013]\n",
      "Time for epoch 1701 is 1.1033563613891602 sec\n",
      "1800 [D loss: 1.076792] [G loss: 0.000000] [D gradient norm: 0.000021]\n",
      "Time for epoch 1801 is 1.0909061431884766 sec\n",
      "1900 [D loss: 1.079071] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 1901 is 1.0257296562194824 sec\n",
      "2000 [D loss: 1.271763] [G loss: 0.000000] [D gradient norm: 0.000021]\n",
      "Time for epoch 2001 is 1.0088913440704346 sec\n",
      "2100 [D loss: 1.509309] [G loss: 0.000000] [D gradient norm: 0.000042]\n",
      "Time for epoch 2101 is 0.9738664627075195 sec\n",
      "2200 [D loss: 1.392553] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 2201 is 1.0711290836334229 sec\n",
      "2300 [D loss: 1.626981] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 2301 is 1.0112214088439941 sec\n",
      "2400 [D loss: 1.072218] [G loss: 0.000000] [D gradient norm: 0.000006]\n",
      "Time for epoch 2401 is 0.9999580383300781 sec\n",
      "2500 [D loss: 1.078538] [G loss: 0.000000] [D gradient norm: 0.000007]\n",
      "Time for epoch 2501 is 0.9891681671142578 sec\n",
      "2600 [D loss: 1.078548] [G loss: 0.000000] [D gradient norm: 0.000106]\n",
      "Time for epoch 2601 is 1.0109009742736816 sec\n",
      "2700 [D loss: 1.541007] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 2701 is 0.9982562065124512 sec\n",
      "2800 [D loss: 1.078162] [G loss: 0.000000] [D gradient norm: 0.000007]\n",
      "Time for epoch 2801 is 0.9928061962127686 sec\n",
      "2900 [D loss: 1.129991] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 2901 is 0.9871227741241455 sec\n",
      "3000 [D loss: 1.138360] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3001 is 0.9595232009887695 sec\n",
      "3100 [D loss: 1.092174] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3101 is 0.9794204235076904 sec\n",
      "3200 [D loss: 1.077254] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3201 is 1.0012857913970947 sec\n",
      "3300 [D loss: 1.077863] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3301 is 1.022202491760254 sec\n",
      "3400 [D loss: 1.525516] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3401 is 0.9788825511932373 sec\n",
      "3500 [D loss: 1.473020] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.9789576530456543 sec\n",
      "3600 [D loss: 1.461016] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3601 is 0.979229211807251 sec\n",
      "3700 [D loss: 1.084991] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.9729576110839844 sec\n",
      "3800 [D loss: 1.105887] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3801 is 0.9917421340942383 sec\n",
      "3900 [D loss: 1.107685] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3901 is 1.0145542621612549 sec\n",
      "4000 [D loss: 1.308673] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4001 is 0.9678678512573242 sec\n",
      "4100 [D loss: 1.281960] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.9859743118286133 sec\n",
      "4200 [D loss: 1.078367] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 4201 is 0.9791498184204102 sec\n",
      "4300 [D loss: 1.076706] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 1.0240209102630615 sec\n",
      "4400 [D loss: 1.179984] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.9835896492004395 sec\n",
      "4500 [D loss: 1.211500] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4501 is 0.993826150894165 sec\n",
      "4600 [D loss: 1.205732] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.9597804546356201 sec\n",
      "4700 [D loss: 1.077075] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4701 is 0.9645168781280518 sec\n",
      "4800 [D loss: 1.128105] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4801 is 1.0277094841003418 sec\n",
      "4900 [D loss: 1.078775] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 1.0234591960906982 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 06:16:47,262] Trial 5 finished with value: 4415.376193610249 and parameters: {'num_layers_generator': 6, 'num_layers_discriminator': 4, 'neurons_per_layer_generator': 128, 'neurons_per_layer_discriminator': 512, 'learning_rate': 3.110557511834719e-06, 'n_critic': 5, 'lambda_gp': 0.07877706809574962, 'latent_dim': 32, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 4415.376193610249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 10.333185] [G loss: 0.437988] [D gradient norm: 0.716830]\n",
      "Time for epoch 1 is 4.485722303390503 sec\n",
      "100 [D loss: 10.792720] [G loss: 0.001932] [D gradient norm: 0.102922]\n",
      "Time for epoch 101 is 0.8637993335723877 sec\n",
      "200 [D loss: 23.473015] [G loss: 0.000354] [D gradient norm: 0.109722]\n",
      "Time for epoch 201 is 0.8645403385162354 sec\n",
      "300 [D loss: 16.202524] [G loss: 0.000016] [D gradient norm: 0.003425]\n",
      "Time for epoch 301 is 0.8324484825134277 sec\n",
      "400 [D loss: 11.843486] [G loss: 0.000001] [D gradient norm: 0.000025]\n",
      "Time for epoch 401 is 0.8269953727722168 sec\n",
      "500 [D loss: 59.434742] [G loss: 0.000000] [D gradient norm: 0.000062]\n",
      "Time for epoch 501 is 0.811314582824707 sec\n",
      "600 [D loss: 11.129947] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 601 is 0.8030180931091309 sec\n",
      "700 [D loss: 23.133112] [G loss: 0.000000] [D gradient norm: 0.000019]\n",
      "Time for epoch 701 is 0.8242480754852295 sec\n",
      "800 [D loss: 49.026722] [G loss: 0.000000] [D gradient norm: 0.000006]\n",
      "Time for epoch 801 is 0.7932286262512207 sec\n",
      "900 [D loss: 45.006317] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 901 is 0.8098297119140625 sec\n",
      "1000 [D loss: 60.652710] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1001 is 0.8614456653594971 sec\n",
      "1100 [D loss: 14.822796] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1101 is 0.8660244941711426 sec\n",
      "1200 [D loss: 39.498795] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 1201 is 0.832643985748291 sec\n",
      "1300 [D loss: 24.734787] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1301 is 0.8120212554931641 sec\n",
      "1400 [D loss: 11.187050] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1401 is 0.8455421924591064 sec\n",
      "1500 [D loss: 10.871033] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1501 is 0.8132472038269043 sec\n",
      "1600 [D loss: 13.203872] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1601 is 0.8484265804290771 sec\n",
      "1700 [D loss: 11.080595] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1701 is 0.848785400390625 sec\n",
      "1800 [D loss: 100.676651] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1801 is 0.8091320991516113 sec\n",
      "1900 [D loss: 10.777041] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1901 is 0.7921261787414551 sec\n",
      "2000 [D loss: 29.238998] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2001 is 0.787006139755249 sec\n",
      "2100 [D loss: 10.761704] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2101 is 0.8201725482940674 sec\n",
      "2200 [D loss: 61.230618] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2201 is 0.8486216068267822 sec\n",
      "2300 [D loss: 62.538685] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2301 is 0.890648365020752 sec\n",
      "2400 [D loss: 44.308872] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2401 is 0.8304216861724854 sec\n",
      "2500 [D loss: 11.262491] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2501 is 0.8618965148925781 sec\n",
      "2600 [D loss: 10.809924] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2601 is 0.8701283931732178 sec\n",
      "2700 [D loss: 68.825203] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2701 is 0.8144164085388184 sec\n",
      "2800 [D loss: 58.841427] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2801 is 0.8068654537200928 sec\n",
      "2900 [D loss: 45.605976] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2901 is 0.85978102684021 sec\n",
      "3000 [D loss: 10.807581] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3001 is 0.8261692523956299 sec\n",
      "3100 [D loss: 11.001319] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3101 is 0.8191308975219727 sec\n",
      "3200 [D loss: 39.843304] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3201 is 0.8099575042724609 sec\n",
      "3300 [D loss: 16.384243] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3301 is 0.875605583190918 sec\n",
      "3400 [D loss: 50.582123] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3401 is 0.8500635623931885 sec\n",
      "3500 [D loss: 31.239260] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.7847981452941895 sec\n",
      "3600 [D loss: 13.020345] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3601 is 0.8603131771087646 sec\n",
      "3700 [D loss: 12.969190] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.7709898948669434 sec\n",
      "3800 [D loss: 67.492401] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3801 is 0.8568940162658691 sec\n",
      "3900 [D loss: 76.477402] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3901 is 0.8086743354797363 sec\n",
      "4000 [D loss: 78.614838] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4001 is 0.8314638137817383 sec\n",
      "4100 [D loss: 18.283424] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.8042197227478027 sec\n",
      "4200 [D loss: 11.218100] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.7941296100616455 sec\n",
      "4300 [D loss: 11.306115] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 0.859250545501709 sec\n",
      "4400 [D loss: 27.099676] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.8323962688446045 sec\n",
      "4500 [D loss: 42.781601] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4501 is 0.8444955348968506 sec\n",
      "4600 [D loss: 46.489006] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.8761160373687744 sec\n",
      "4700 [D loss: 10.822694] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4701 is 0.8156545162200928 sec\n",
      "4800 [D loss: 38.142632] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4801 is 0.8043034076690674 sec\n",
      "4900 [D loss: 11.297060] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.8161139488220215 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 07:25:35,728] Trial 6 finished with value: 5090.238723918495 and parameters: {'num_layers_generator': 6, 'num_layers_discriminator': 4, 'neurons_per_layer_generator': 512, 'neurons_per_layer_discriminator': 512, 'learning_rate': 1.428737669505527e-05, 'n_critic': 4, 'lambda_gp': 10.306137217415335, 'latent_dim': 95, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 4415.376193610249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.935544] [G loss: 0.434515] [D gradient norm: 0.671306]\n",
      "Time for epoch 1 is 3.5444836616516113 sec\n",
      "100 [D loss: 0.894508] [G loss: 0.395067] [D gradient norm: 0.648805]\n",
      "Time for epoch 101 is 0.40265560150146484 sec\n",
      "200 [D loss: 0.863465] [G loss: 0.358279] [D gradient norm: 0.618158]\n",
      "Time for epoch 201 is 0.38143134117126465 sec\n",
      "300 [D loss: 0.829883] [G loss: 0.322375] [D gradient norm: 0.602530]\n",
      "Time for epoch 301 is 0.319138765335083 sec\n",
      "400 [D loss: 0.805304] [G loss: 0.287723] [D gradient norm: 0.566201]\n",
      "Time for epoch 401 is 0.3707878589630127 sec\n",
      "500 [D loss: 0.765453] [G loss: 0.256347] [D gradient norm: 0.535606]\n",
      "Time for epoch 501 is 0.37186431884765625 sec\n",
      "600 [D loss: 0.702712] [G loss: 0.226148] [D gradient norm: 0.487523]\n",
      "Time for epoch 601 is 0.32935476303100586 sec\n",
      "700 [D loss: 0.698655] [G loss: 0.199647] [D gradient norm: 0.457182]\n",
      "Time for epoch 701 is 0.37409353256225586 sec\n",
      "800 [D loss: 0.672522] [G loss: 0.176282] [D gradient norm: 0.414272]\n",
      "Time for epoch 801 is 0.37299585342407227 sec\n",
      "900 [D loss: 0.648276] [G loss: 0.152764] [D gradient norm: 0.383592]\n",
      "Time for epoch 901 is 0.3661336898803711 sec\n",
      "1000 [D loss: 0.628057] [G loss: 0.132992] [D gradient norm: 0.344903]\n",
      "Time for epoch 1001 is 0.3969283103942871 sec\n",
      "1100 [D loss: 0.624014] [G loss: 0.114332] [D gradient norm: 0.320359]\n",
      "Time for epoch 1101 is 0.35849547386169434 sec\n",
      "1200 [D loss: 0.615475] [G loss: 0.098830] [D gradient norm: 0.281810]\n",
      "Time for epoch 1201 is 0.3466670513153076 sec\n",
      "1300 [D loss: 0.580407] [G loss: 0.083788] [D gradient norm: 0.247033]\n",
      "Time for epoch 1301 is 0.37638401985168457 sec\n",
      "1400 [D loss: 0.576983] [G loss: 0.070343] [D gradient norm: 0.220181]\n",
      "Time for epoch 1401 is 0.3628859519958496 sec\n",
      "1500 [D loss: 0.576925] [G loss: 0.060128] [D gradient norm: 0.204296]\n",
      "Time for epoch 1501 is 0.3592543601989746 sec\n",
      "1600 [D loss: 0.538616] [G loss: 0.049712] [D gradient norm: 0.164875]\n",
      "Time for epoch 1601 is 0.3521263599395752 sec\n",
      "1700 [D loss: 0.556601] [G loss: 0.041129] [D gradient norm: 0.150648]\n",
      "Time for epoch 1701 is 0.35683345794677734 sec\n",
      "1800 [D loss: 0.535751] [G loss: 0.035453] [D gradient norm: 0.121063]\n",
      "Time for epoch 1801 is 0.3493838310241699 sec\n",
      "1900 [D loss: 0.548594] [G loss: 0.029788] [D gradient norm: 0.115097]\n",
      "Time for epoch 1901 is 0.3697168827056885 sec\n",
      "2000 [D loss: 0.539525] [G loss: 0.024954] [D gradient norm: 0.096998]\n",
      "Time for epoch 2001 is 0.33578062057495117 sec\n",
      "2100 [D loss: 0.528624] [G loss: 0.021342] [D gradient norm: 0.089247]\n",
      "Time for epoch 2101 is 0.3282153606414795 sec\n",
      "2200 [D loss: 0.505187] [G loss: 0.018358] [D gradient norm: 0.069988]\n",
      "Time for epoch 2201 is 0.3579282760620117 sec\n",
      "2300 [D loss: 0.540205] [G loss: 0.015814] [D gradient norm: 0.069529]\n",
      "Time for epoch 2301 is 0.34800100326538086 sec\n",
      "2400 [D loss: 0.546055] [G loss: 0.013517] [D gradient norm: 0.062951]\n",
      "Time for epoch 2401 is 0.3373901844024658 sec\n",
      "2500 [D loss: 0.514911] [G loss: 0.011902] [D gradient norm: 0.050272]\n",
      "Time for epoch 2501 is 0.36002588272094727 sec\n",
      "2600 [D loss: 0.508142] [G loss: 0.010559] [D gradient norm: 0.045573]\n",
      "Time for epoch 2601 is 0.3425023555755615 sec\n",
      "2700 [D loss: 0.475544] [G loss: 0.009434] [D gradient norm: 0.039365]\n",
      "Time for epoch 2701 is 0.364687442779541 sec\n",
      "2800 [D loss: 0.518049] [G loss: 0.008666] [D gradient norm: 0.038765]\n",
      "Time for epoch 2801 is 0.3399507999420166 sec\n",
      "2900 [D loss: 0.549895] [G loss: 0.008009] [D gradient norm: 0.039245]\n",
      "Time for epoch 2901 is 0.3552381992340088 sec\n",
      "3000 [D loss: 0.564254] [G loss: 0.007398] [D gradient norm: 0.036167]\n",
      "Time for epoch 3001 is 0.3590395450592041 sec\n",
      "3100 [D loss: 0.545113] [G loss: 0.006960] [D gradient norm: 0.033660]\n",
      "Time for epoch 3101 is 0.36380457878112793 sec\n",
      "3200 [D loss: 0.550108] [G loss: 0.006404] [D gradient norm: 0.033023]\n",
      "Time for epoch 3201 is 0.3619985580444336 sec\n",
      "3300 [D loss: 0.522119] [G loss: 0.006208] [D gradient norm: 0.031146]\n",
      "Time for epoch 3301 is 0.3461952209472656 sec\n",
      "3400 [D loss: 0.555559] [G loss: 0.005817] [D gradient norm: 0.031918]\n",
      "Time for epoch 3401 is 0.3551969528198242 sec\n",
      "3500 [D loss: 0.566198] [G loss: 0.005786] [D gradient norm: 0.029866]\n",
      "Time for epoch 3501 is 0.378490686416626 sec\n",
      "3600 [D loss: 0.523484] [G loss: 0.005454] [D gradient norm: 0.026850]\n",
      "Time for epoch 3601 is 0.36435413360595703 sec\n",
      "3700 [D loss: 0.567226] [G loss: 0.005348] [D gradient norm: 0.028916]\n",
      "Time for epoch 3701 is 0.35256171226501465 sec\n",
      "3800 [D loss: 0.566915] [G loss: 0.005224] [D gradient norm: 0.029865]\n",
      "Time for epoch 3801 is 0.34996771812438965 sec\n",
      "3900 [D loss: 0.553588] [G loss: 0.005176] [D gradient norm: 0.027730]\n",
      "Time for epoch 3901 is 0.3429248332977295 sec\n",
      "4000 [D loss: 0.580768] [G loss: 0.004988] [D gradient norm: 0.027305]\n",
      "Time for epoch 4001 is 0.3619725704193115 sec\n",
      "4100 [D loss: 0.587070] [G loss: 0.004987] [D gradient norm: 0.028273]\n",
      "Time for epoch 4101 is 0.3762855529785156 sec\n",
      "4200 [D loss: 0.557971] [G loss: 0.004896] [D gradient norm: 0.026949]\n",
      "Time for epoch 4201 is 0.3540487289428711 sec\n",
      "4300 [D loss: 0.612979] [G loss: 0.004972] [D gradient norm: 0.032301]\n",
      "Time for epoch 4301 is 0.36113905906677246 sec\n",
      "4400 [D loss: 0.599195] [G loss: 0.004896] [D gradient norm: 0.031382]\n",
      "Time for epoch 4401 is 0.3526725769042969 sec\n",
      "4500 [D loss: 0.600294] [G loss: 0.004840] [D gradient norm: 0.030587]\n",
      "Time for epoch 4501 is 0.3469715118408203 sec\n",
      "4600 [D loss: 0.628785] [G loss: 0.004821] [D gradient norm: 0.033422]\n",
      "Time for epoch 4601 is 0.3364744186401367 sec\n",
      "4700 [D loss: 0.635257] [G loss: 0.004870] [D gradient norm: 0.032614]\n",
      "Time for epoch 4701 is 0.34485363960266113 sec\n",
      "4800 [D loss: 0.607997] [G loss: 0.004834] [D gradient norm: 0.029346]\n",
      "Time for epoch 4801 is 0.35166120529174805 sec\n",
      "4900 [D loss: 0.625832] [G loss: 0.004904] [D gradient norm: 0.033249]\n",
      "Time for epoch 4901 is 0.3661346435546875 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 07:55:31,962] Trial 7 finished with value: 5494.705372067012 and parameters: {'num_layers_generator': 5, 'num_layers_discriminator': 2, 'neurons_per_layer_generator': 128, 'neurons_per_layer_discriminator': 64, 'learning_rate': 2.331951479780438e-06, 'n_critic': 2, 'lambda_gp': 0.016496801135832226, 'latent_dim': 66, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 4415.376193610249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.100684] [G loss: 0.492680] [D gradient norm: 0.832367]\n",
      "Time for epoch 1 is 2.050595760345459 sec\n",
      "100 [D loss: 1.136032] [G loss: 0.000053] [D gradient norm: 0.023361]\n",
      "Time for epoch 101 is 0.22547006607055664 sec\n",
      "200 [D loss: 1.110215] [G loss: 0.000016] [D gradient norm: 0.025525]\n",
      "Time for epoch 201 is 0.23187851905822754 sec\n",
      "300 [D loss: 1.381542] [G loss: 0.000011] [D gradient norm: 0.004308]\n",
      "Time for epoch 301 is 0.2270221710205078 sec\n",
      "400 [D loss: 1.192092] [G loss: 0.000005] [D gradient norm: 0.000401]\n",
      "Time for epoch 401 is 0.24060773849487305 sec\n",
      "500 [D loss: 1.073550] [G loss: 0.000002] [D gradient norm: 0.000175]\n",
      "Time for epoch 501 is 0.2041609287261963 sec\n",
      "600 [D loss: 1.074435] [G loss: 0.000003] [D gradient norm: 0.000205]\n",
      "Time for epoch 601 is 0.21115970611572266 sec\n",
      "700 [D loss: 1.072444] [G loss: 0.000002] [D gradient norm: 0.000121]\n",
      "Time for epoch 701 is 0.20920085906982422 sec\n",
      "800 [D loss: 1.174328] [G loss: 0.000002] [D gradient norm: 0.000091]\n",
      "Time for epoch 801 is 0.2055065631866455 sec\n",
      "900 [D loss: 1.076076] [G loss: 0.000001] [D gradient norm: 0.000082]\n",
      "Time for epoch 901 is 0.2050151824951172 sec\n",
      "1000 [D loss: 1.595091] [G loss: 0.000004] [D gradient norm: 0.000119]\n",
      "Time for epoch 1001 is 0.20038151741027832 sec\n",
      "1100 [D loss: 1.077628] [G loss: 0.000001] [D gradient norm: 0.000097]\n",
      "Time for epoch 1101 is 0.20016193389892578 sec\n",
      "1200 [D loss: 1.203218] [G loss: 0.000002] [D gradient norm: 0.000103]\n",
      "Time for epoch 1201 is 0.21778106689453125 sec\n",
      "1300 [D loss: 1.217750] [G loss: 0.000001] [D gradient norm: 0.000038]\n",
      "Time for epoch 1301 is 0.2111351490020752 sec\n",
      "1400 [D loss: 1.182095] [G loss: 0.000002] [D gradient norm: 0.000039]\n",
      "Time for epoch 1401 is 0.2011103630065918 sec\n",
      "1500 [D loss: 1.092115] [G loss: 0.000001] [D gradient norm: 0.000054]\n",
      "Time for epoch 1501 is 0.19512200355529785 sec\n",
      "1600 [D loss: 1.104558] [G loss: 0.000002] [D gradient norm: 0.000053]\n",
      "Time for epoch 1601 is 0.19955015182495117 sec\n",
      "1700 [D loss: 1.098111] [G loss: 0.000001] [D gradient norm: 0.000050]\n",
      "Time for epoch 1701 is 0.1768321990966797 sec\n",
      "1800 [D loss: 1.205153] [G loss: 0.000000] [D gradient norm: 0.000023]\n",
      "Time for epoch 1801 is 0.1986546516418457 sec\n",
      "1900 [D loss: 1.234868] [G loss: 0.000000] [D gradient norm: 0.000036]\n",
      "Time for epoch 1901 is 0.22043251991271973 sec\n",
      "2000 [D loss: 1.075071] [G loss: 0.000001] [D gradient norm: 0.000021]\n",
      "Time for epoch 2001 is 0.20634698867797852 sec\n",
      "2100 [D loss: 1.219621] [G loss: 0.000001] [D gradient norm: 0.000064]\n",
      "Time for epoch 2101 is 0.22406482696533203 sec\n",
      "2200 [D loss: 1.087570] [G loss: 0.000000] [D gradient norm: 0.000019]\n",
      "Time for epoch 2201 is 0.2136218547821045 sec\n",
      "2300 [D loss: 1.210997] [G loss: 0.000000] [D gradient norm: 0.000019]\n",
      "Time for epoch 2301 is 0.20321989059448242 sec\n",
      "2400 [D loss: 1.079441] [G loss: 0.000000] [D gradient norm: 0.000032]\n",
      "Time for epoch 2401 is 0.1953890323638916 sec\n",
      "2500 [D loss: 1.077260] [G loss: 0.000000] [D gradient norm: 0.000017]\n",
      "Time for epoch 2501 is 0.21996021270751953 sec\n",
      "2600 [D loss: 1.078537] [G loss: 0.000000] [D gradient norm: 0.000007]\n",
      "Time for epoch 2601 is 0.22488832473754883 sec\n",
      "2700 [D loss: 1.079391] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 2701 is 0.1955885887145996 sec\n",
      "2800 [D loss: 1.247650] [G loss: 0.000000] [D gradient norm: 0.000008]\n",
      "Time for epoch 2801 is 0.2127223014831543 sec\n",
      "2900 [D loss: 1.196455] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 2901 is 0.20611906051635742 sec\n",
      "3000 [D loss: 1.074297] [G loss: 0.000000] [D gradient norm: 0.000015]\n",
      "Time for epoch 3001 is 0.19717788696289062 sec\n",
      "3100 [D loss: 1.281775] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3101 is 0.18937015533447266 sec\n",
      "3200 [D loss: 1.156225] [G loss: 0.000000] [D gradient norm: 0.000009]\n",
      "Time for epoch 3201 is 0.2111988067626953 sec\n",
      "3300 [D loss: 1.151716] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3301 is 0.1940915584564209 sec\n",
      "3400 [D loss: 1.078316] [G loss: 0.000000] [D gradient norm: 0.000006]\n",
      "Time for epoch 3401 is 0.1979525089263916 sec\n",
      "3500 [D loss: 1.077176] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3501 is 0.23853206634521484 sec\n",
      "3600 [D loss: 1.858819] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 3601 is 0.211503267288208 sec\n",
      "3700 [D loss: 1.078090] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3701 is 0.21173667907714844 sec\n",
      "3800 [D loss: 1.077173] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 3801 is 0.19807744026184082 sec\n",
      "3900 [D loss: 1.075012] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3901 is 0.21394920349121094 sec\n",
      "4000 [D loss: 1.078499] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4001 is 0.22113251686096191 sec\n",
      "4100 [D loss: 1.276713] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4101 is 0.2162020206451416 sec\n",
      "4200 [D loss: 1.079115] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4201 is 0.22122478485107422 sec\n",
      "4300 [D loss: 1.136748] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4301 is 0.20192360877990723 sec\n",
      "4400 [D loss: 1.118307] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 4401 is 0.20670700073242188 sec\n",
      "4500 [D loss: 1.124387] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 4501 is 0.22910594940185547 sec\n",
      "4600 [D loss: 1.085029] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 4601 is 0.21018505096435547 sec\n",
      "4700 [D loss: 1.077812] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4701 is 0.19108033180236816 sec\n",
      "4800 [D loss: 1.090040] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4801 is 0.19580578804016113 sec\n",
      "4900 [D loss: 1.077208] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4901 is 0.19735455513000488 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 08:13:11,739] Trial 8 finished with value: 5337.783171805451 and parameters: {'num_layers_generator': 3, 'num_layers_discriminator': 4, 'neurons_per_layer_generator': 256, 'neurons_per_layer_discriminator': 512, 'learning_rate': 0.00010856594899009799, 'n_critic': 1, 'lambda_gp': 0.07894567396833398, 'latent_dim': 97, 'optimizer': 'Adam'}. Best is trial 5 with value: 4415.376193610249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 23.686022] [G loss: 0.490660] [D gradient norm: 0.518767]\n",
      "Time for epoch 1 is 2.5416150093078613 sec\n",
      "100 [D loss: 21.371746] [G loss: 0.007366] [D gradient norm: 0.073022]\n",
      "Time for epoch 101 is 0.5227973461151123 sec\n",
      "200 [D loss: 19.505033] [G loss: 0.006222] [D gradient norm: 0.172331]\n",
      "Time for epoch 201 is 0.5787222385406494 sec\n",
      "300 [D loss: 42.156082] [G loss: 0.001467] [D gradient norm: 0.212270]\n",
      "Time for epoch 301 is 0.5826003551483154 sec\n",
      "400 [D loss: 36.687771] [G loss: 0.000500] [D gradient norm: 0.043962]\n",
      "Time for epoch 401 is 0.5379369258880615 sec\n",
      "500 [D loss: 22.914030] [G loss: 0.000194] [D gradient norm: 0.024043]\n",
      "Time for epoch 501 is 0.6078164577484131 sec\n",
      "600 [D loss: 103.984726] [G loss: 0.000139] [D gradient norm: 0.017169]\n",
      "Time for epoch 601 is 0.5749213695526123 sec\n",
      "700 [D loss: 94.532669] [G loss: 0.000073] [D gradient norm: 0.009030]\n",
      "Time for epoch 701 is 0.5621469020843506 sec\n",
      "800 [D loss: 72.500847] [G loss: 0.000024] [D gradient norm: 0.004110]\n",
      "Time for epoch 801 is 0.548321008682251 sec\n",
      "900 [D loss: 35.808765] [G loss: 0.000039] [D gradient norm: 0.008456]\n",
      "Time for epoch 901 is 0.5842938423156738 sec\n",
      "1000 [D loss: 67.136772] [G loss: 0.000032] [D gradient norm: 0.001053]\n",
      "Time for epoch 1001 is 0.5722408294677734 sec\n",
      "1100 [D loss: 40.995697] [G loss: 0.000011] [D gradient norm: 0.000778]\n",
      "Time for epoch 1101 is 0.5443704128265381 sec\n",
      "1200 [D loss: 33.667664] [G loss: 0.000031] [D gradient norm: 0.001013]\n",
      "Time for epoch 1201 is 0.5733623504638672 sec\n",
      "1300 [D loss: 147.718948] [G loss: 0.000005] [D gradient norm: 0.000319]\n",
      "Time for epoch 1301 is 0.5520973205566406 sec\n",
      "1400 [D loss: 24.540760] [G loss: 0.000011] [D gradient norm: 0.000708]\n",
      "Time for epoch 1401 is 0.5449082851409912 sec\n",
      "1500 [D loss: 36.069534] [G loss: 0.000002] [D gradient norm: 0.000217]\n",
      "Time for epoch 1501 is 0.49512171745300293 sec\n",
      "1600 [D loss: 147.569717] [G loss: 0.000001] [D gradient norm: 0.000119]\n",
      "Time for epoch 1601 is 0.5532922744750977 sec\n",
      "1700 [D loss: 123.690331] [G loss: 0.000001] [D gradient norm: 0.000164]\n",
      "Time for epoch 1701 is 0.5256555080413818 sec\n",
      "1800 [D loss: 31.748440] [G loss: 0.000000] [D gradient norm: 0.000175]\n",
      "Time for epoch 1801 is 0.4990720748901367 sec\n",
      "1900 [D loss: 134.319839] [G loss: 0.000002] [D gradient norm: 0.000296]\n",
      "Time for epoch 1901 is 0.570565938949585 sec\n",
      "2000 [D loss: 23.914427] [G loss: 0.000000] [D gradient norm: 0.000020]\n",
      "Time for epoch 2001 is 0.566831111907959 sec\n",
      "2100 [D loss: 479.421448] [G loss: 0.000000] [D gradient norm: 0.000026]\n",
      "Time for epoch 2101 is 0.5807676315307617 sec\n",
      "2200 [D loss: 25.274782] [G loss: 0.000000] [D gradient norm: 0.000005]\n",
      "Time for epoch 2201 is 0.5603570938110352 sec\n",
      "2300 [D loss: 45.658089] [G loss: 0.000000] [D gradient norm: 0.000027]\n",
      "Time for epoch 2301 is 0.5591745376586914 sec\n",
      "2400 [D loss: 63.737103] [G loss: 0.000000] [D gradient norm: 0.000556]\n",
      "Time for epoch 2401 is 0.5328423976898193 sec\n",
      "2500 [D loss: 24.788031] [G loss: 0.000000] [D gradient norm: 0.000005]\n",
      "Time for epoch 2501 is 0.552396297454834 sec\n",
      "2600 [D loss: 30.789688] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 2601 is 0.5918323993682861 sec\n",
      "2700 [D loss: 24.207212] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 2701 is 0.5683906078338623 sec\n",
      "2800 [D loss: 25.437977] [G loss: 0.000000] [D gradient norm: 0.000014]\n",
      "Time for epoch 2801 is 0.5638575553894043 sec\n",
      "2900 [D loss: 24.877474] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 2901 is 0.5725719928741455 sec\n",
      "3000 [D loss: 69.425949] [G loss: 0.000000] [D gradient norm: 0.000022]\n",
      "Time for epoch 3001 is 0.5607352256774902 sec\n",
      "3100 [D loss: 24.701845] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3101 is 0.5662670135498047 sec\n",
      "3200 [D loss: 25.238041] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3201 is 0.6094636917114258 sec\n",
      "3300 [D loss: 24.935257] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3301 is 0.5753154754638672 sec\n",
      "3400 [D loss: 305.182373] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3401 is 0.5591707229614258 sec\n",
      "3500 [D loss: 186.222519] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 3501 is 0.5613400936126709 sec\n",
      "3600 [D loss: 240.616028] [G loss: 0.000000] [D gradient norm: 0.000002]\n",
      "Time for epoch 3601 is 0.5597083568572998 sec\n",
      "3700 [D loss: 69.152771] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 3701 is 0.5518219470977783 sec\n",
      "3800 [D loss: 23.564592] [G loss: 0.000000] [D gradient norm: 0.000004]\n",
      "Time for epoch 3801 is 0.543701171875 sec\n",
      "3900 [D loss: 32.688442] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 3901 is 0.5686514377593994 sec\n",
      "4000 [D loss: 244.890503] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4001 is 0.5509395599365234 sec\n",
      "4100 [D loss: 25.373928] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4101 is 0.5592377185821533 sec\n",
      "4200 [D loss: 321.963837] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.5506951808929443 sec\n",
      "4300 [D loss: 89.258682] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 0.5880272388458252 sec\n",
      "4400 [D loss: 23.922762] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.5712385177612305 sec\n",
      "4500 [D loss: 24.739742] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4501 is 0.5546607971191406 sec\n",
      "4600 [D loss: 385.299988] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.5396876335144043 sec\n",
      "4700 [D loss: 197.228394] [G loss: 0.000000] [D gradient norm: 0.000003]\n",
      "Time for epoch 4701 is 0.5820379257202148 sec\n",
      "4800 [D loss: 25.189774] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 4801 is 0.6014509201049805 sec\n",
      "4900 [D loss: 80.689896] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.5549666881561279 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 09:00:25,774] Trial 9 finished with value: 5119.92399244417 and parameters: {'num_layers_generator': 3, 'num_layers_discriminator': 6, 'neurons_per_layer_generator': 128, 'neurons_per_layer_discriminator': 128, 'learning_rate': 1.5285604924261202e-05, 'n_critic': 3, 'lambda_gp': 24.503750193751088, 'latent_dim': 89, 'optimizer': 'Adam'}. Best is trial 5 with value: 4415.376193610249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.482386] [G loss: 0.003403] [D gradient norm: 0.014047]\n",
      "Time for epoch 1 is 3.242969274520874 sec\n",
      "100 [D loss: 1.048793] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 101 is 0.8271353244781494 sec\n",
      "200 [D loss: 1.014024] [G loss: 0.000000] [D gradient norm: 0.000001]\n",
      "Time for epoch 201 is 0.8284611701965332 sec\n",
      "300 [D loss: 1.014029] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 301 is 0.8193504810333252 sec\n",
      "400 [D loss: 1.014158] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 401 is 0.8576939105987549 sec\n",
      "500 [D loss: 1.013893] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 501 is 0.8014309406280518 sec\n",
      "600 [D loss: 1.065755] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 601 is 0.8680765628814697 sec\n",
      "700 [D loss: 1.016667] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 701 is 0.8374309539794922 sec\n",
      "800 [D loss: 1.014499] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 801 is 0.8429608345031738 sec\n",
      "900 [D loss: 1.034883] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 901 is 0.7805492877960205 sec\n",
      "1000 [D loss: 1.131923] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1001 is 0.8155946731567383 sec\n",
      "1100 [D loss: 1.030379] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1101 is 0.8191015720367432 sec\n",
      "1200 [D loss: 1.114928] [G loss: 0.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1201 is 0.8411409854888916 sec\n",
      "1300 [D loss: 2.014171] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1301 is 0.8495440483093262 sec\n",
      "1400 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1401 is 0.7870268821716309 sec\n",
      "1500 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1501 is 0.8263890743255615 sec\n",
      "1600 [D loss: 2.086243] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1601 is 0.859022855758667 sec\n",
      "1700 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1701 is 0.8066856861114502 sec\n",
      "1800 [D loss: 2.014116] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1801 is 0.8537266254425049 sec\n",
      "1900 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 1901 is 0.8115367889404297 sec\n",
      "2000 [D loss: 2.014125] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2001 is 0.8197379112243652 sec\n",
      "2100 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2101 is 0.78757643699646 sec\n",
      "2200 [D loss: 2.020929] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2201 is 0.7910768985748291 sec\n",
      "2300 [D loss: 2.178481] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2301 is 0.8207688331604004 sec\n",
      "2400 [D loss: 2.014176] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2401 is 0.7984991073608398 sec\n",
      "2500 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2501 is 0.806830883026123 sec\n",
      "2600 [D loss: 2.013782] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2601 is 0.8183145523071289 sec\n",
      "2700 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2701 is 0.7851476669311523 sec\n",
      "2800 [D loss: 2.104400] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2801 is 0.8051328659057617 sec\n",
      "2900 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 2901 is 0.7751731872558594 sec\n",
      "3000 [D loss: 2.014170] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3001 is 0.823723316192627 sec\n",
      "3100 [D loss: 2.014175] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3101 is 0.8335599899291992 sec\n",
      "3200 [D loss: 2.202647] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3201 is 0.8370711803436279 sec\n",
      "3300 [D loss: 2.014143] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3301 is 0.8357458114624023 sec\n",
      "3400 [D loss: 2.014177] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3401 is 0.7560985088348389 sec\n",
      "3500 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3501 is 0.7877211570739746 sec\n",
      "3600 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3601 is 0.7998483180999756 sec\n",
      "3700 [D loss: 2.060561] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3701 is 0.7806856632232666 sec\n",
      "3800 [D loss: 2.014175] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3801 is 0.8752658367156982 sec\n",
      "3900 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 3901 is 0.8091647624969482 sec\n",
      "4000 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4001 is 0.8036441802978516 sec\n",
      "4100 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4101 is 0.8237001895904541 sec\n",
      "4200 [D loss: 2.014177] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4201 is 0.8789753913879395 sec\n",
      "4300 [D loss: 2.014176] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4301 is 0.7974934577941895 sec\n",
      "4400 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4401 is 0.791759729385376 sec\n",
      "4500 [D loss: 2.013655] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4501 is 0.7367391586303711 sec\n",
      "4600 [D loss: 2.014168] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4601 is 0.8230819702148438 sec\n",
      "4700 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4701 is 0.8056104183197021 sec\n",
      "4800 [D loss: 2.014178] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4801 is 0.7852895259857178 sec\n",
      "4900 [D loss: 2.014176] [G loss: 1.000000] [D gradient norm: 0.000000]\n",
      "Time for epoch 4901 is 0.8050475120544434 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 10:08:24,615] Trial 10 finished with value: 6004.441256929987 and parameters: {'num_layers_generator': 6, 'num_layers_discriminator': 2, 'neurons_per_layer_generator': 128, 'neurons_per_layer_discriminator': 512, 'learning_rate': 0.0009037492214450291, 'n_critic': 5, 'lambda_gp': 0.014177741754022269, 'latent_dim': 32, 'optimizer': 'Adam'}. Best is trial 5 with value: 4415.376193610249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 6.476118] [G loss: 0.456582] [D gradient norm: 0.582217]\n",
      "Time for epoch 1 is 4.892173767089844 sec\n",
      "100 [D loss: 6.145654] [G loss: 0.326115] [D gradient norm: 0.564590]\n",
      "Time for epoch 101 is 0.8150250911712646 sec\n",
      "200 [D loss: 5.992517] [G loss: 0.184608] [D gradient norm: 0.497817]\n",
      "Time for epoch 201 is 0.7939791679382324 sec\n",
      "300 [D loss: 6.044525] [G loss: 0.073966] [D gradient norm: 0.310507]\n",
      "Time for epoch 301 is 0.792595624923706 sec\n",
      "400 [D loss: 5.671844] [G loss: 0.022487] [D gradient norm: 0.119827]\n",
      "Time for epoch 401 is 0.8075876235961914 sec\n",
      "500 [D loss: 5.912882] [G loss: 0.008767] [D gradient norm: 0.057198]\n",
      "Time for epoch 501 is 0.8475444316864014 sec\n",
      "600 [D loss: 5.529698] [G loss: 0.005725] [D gradient norm: 0.048124]\n",
      "Time for epoch 601 is 0.8134689331054688 sec\n",
      "700 [D loss: 5.755090] [G loss: 0.004682] [D gradient norm: 0.035590]\n",
      "Time for epoch 701 is 0.8325965404510498 sec\n",
      "800 [D loss: 5.831905] [G loss: 0.004220] [D gradient norm: 0.040941]\n",
      "Time for epoch 801 is 0.8351759910583496 sec\n",
      "900 [D loss: 5.685337] [G loss: 0.004233] [D gradient norm: 0.047119]\n",
      "Time for epoch 901 is 0.8163230419158936 sec\n",
      "1000 [D loss: 5.647190] [G loss: 0.004212] [D gradient norm: 0.045475]\n",
      "Time for epoch 1001 is 0.8360016345977783 sec\n",
      "1100 [D loss: 5.877367] [G loss: 0.004661] [D gradient norm: 0.076834]\n",
      "Time for epoch 1101 is 0.8024930953979492 sec\n",
      "1200 [D loss: 5.616045] [G loss: 0.005289] [D gradient norm: 0.130286]\n",
      "Time for epoch 1201 is 0.8088436126708984 sec\n",
      "1300 [D loss: 5.541029] [G loss: 0.004833] [D gradient norm: 0.165061]\n",
      "Time for epoch 1301 is 0.7693030834197998 sec\n",
      "1400 [D loss: 5.745053] [G loss: 0.004828] [D gradient norm: 0.192447]\n",
      "Time for epoch 1401 is 0.7806987762451172 sec\n",
      "1500 [D loss: 6.591988] [G loss: 0.003695] [D gradient norm: 0.194131]\n",
      "Time for epoch 1501 is 0.8713972568511963 sec\n",
      "1600 [D loss: 7.387592] [G loss: 0.003748] [D gradient norm: 0.263594]\n",
      "Time for epoch 1601 is 0.7788619995117188 sec\n",
      "1700 [D loss: 11.024137] [G loss: 0.001365] [D gradient norm: 0.233354]\n",
      "Time for epoch 1701 is 0.7958884239196777 sec\n",
      "1800 [D loss: 11.554959] [G loss: 0.002567] [D gradient norm: 0.141476]\n",
      "Time for epoch 1801 is 0.773728609085083 sec\n",
      "1900 [D loss: 9.330053] [G loss: 0.000917] [D gradient norm: 0.208756]\n",
      "Time for epoch 1901 is 0.7953891754150391 sec\n",
      "2000 [D loss: 7.230202] [G loss: 0.000773] [D gradient norm: 0.053945]\n",
      "Time for epoch 2001 is 0.8072283267974854 sec\n",
      "2100 [D loss: 13.516695] [G loss: 0.000905] [D gradient norm: 0.116498]\n",
      "Time for epoch 2101 is 0.7929685115814209 sec\n",
      "2200 [D loss: 14.028849] [G loss: 0.000476] [D gradient norm: 0.172920]\n",
      "Time for epoch 2201 is 0.7944157123565674 sec\n",
      "2300 [D loss: 21.218792] [G loss: 0.000971] [D gradient norm: 0.100288]\n",
      "Time for epoch 2301 is 0.798229455947876 sec\n",
      "2400 [D loss: 36.317200] [G loss: 0.000421] [D gradient norm: 0.170178]\n",
      "Time for epoch 2401 is 0.8436481952667236 sec\n",
      "2500 [D loss: 9.580711] [G loss: 0.006463] [D gradient norm: 0.027113]\n",
      "Time for epoch 2501 is 0.8066842555999756 sec\n",
      "2600 [D loss: 23.292482] [G loss: 0.000289] [D gradient norm: 0.016964]\n",
      "Time for epoch 2601 is 0.7928693294525146 sec\n",
      "2700 [D loss: 18.331028] [G loss: 0.000308] [D gradient norm: 0.024684]\n",
      "Time for epoch 2701 is 0.7945547103881836 sec\n",
      "2800 [D loss: 7.661776] [G loss: 0.000371] [D gradient norm: 0.014995]\n",
      "Time for epoch 2801 is 0.8110899925231934 sec\n",
      "2900 [D loss: 7.140218] [G loss: 0.000065] [D gradient norm: 0.041816]\n",
      "Time for epoch 2901 is 0.8224649429321289 sec\n",
      "3000 [D loss: 12.599963] [G loss: 0.000200] [D gradient norm: 0.073293]\n",
      "Time for epoch 3001 is 0.8411622047424316 sec\n",
      "3100 [D loss: 36.823689] [G loss: 0.000044] [D gradient norm: 0.004081]\n",
      "Time for epoch 3101 is 0.8108088970184326 sec\n",
      "3200 [D loss: 29.990702] [G loss: 0.000048] [D gradient norm: 0.010728]\n",
      "Time for epoch 3201 is 0.8306272029876709 sec\n",
      "3300 [D loss: 24.854687] [G loss: 0.000017] [D gradient norm: 0.020351]\n",
      "Time for epoch 3301 is 0.854849100112915 sec\n",
      "3400 [D loss: 25.736620] [G loss: 0.000009] [D gradient norm: 0.014067]\n",
      "Time for epoch 3401 is 0.7973530292510986 sec\n",
      "3500 [D loss: 27.374701] [G loss: 0.000058] [D gradient norm: 0.009939]\n",
      "Time for epoch 3501 is 0.8045830726623535 sec\n",
      "3600 [D loss: 25.607977] [G loss: 0.000020] [D gradient norm: 0.004192]\n",
      "Time for epoch 3601 is 0.7894747257232666 sec\n",
      "3700 [D loss: 20.702772] [G loss: 0.000014] [D gradient norm: 0.005320]\n",
      "Time for epoch 3701 is 0.8545207977294922 sec\n",
      "3800 [D loss: 11.515769] [G loss: 0.000006] [D gradient norm: 0.001073]\n",
      "Time for epoch 3801 is 0.8031511306762695 sec\n",
      "3900 [D loss: 43.650490] [G loss: 0.000142] [D gradient norm: 0.001263]\n",
      "Time for epoch 3901 is 0.8595643043518066 sec\n",
      "4000 [D loss: 34.964348] [G loss: 0.000029] [D gradient norm: 0.051349]\n",
      "Time for epoch 4001 is 0.8035571575164795 sec\n",
      "4100 [D loss: 30.353292] [G loss: 0.000002] [D gradient norm: 0.014549]\n",
      "Time for epoch 4101 is 0.8319029808044434 sec\n",
      "4200 [D loss: 28.508595] [G loss: 0.000115] [D gradient norm: 0.004777]\n",
      "Time for epoch 4201 is 0.7879431247711182 sec\n",
      "4300 [D loss: 28.054325] [G loss: 0.000007] [D gradient norm: 0.061163]\n",
      "Time for epoch 4301 is 0.8359634876251221 sec\n",
      "4400 [D loss: 26.211628] [G loss: 0.000003] [D gradient norm: 0.001068]\n",
      "Time for epoch 4401 is 0.8522026538848877 sec\n",
      "4500 [D loss: 36.561581] [G loss: 0.000005] [D gradient norm: 0.000344]\n",
      "Time for epoch 4501 is 0.8052463531494141 sec\n",
      "4600 [D loss: 6.635505] [G loss: 0.000497] [D gradient norm: 0.000206]\n",
      "Time for epoch 4601 is 0.8587093353271484 sec\n",
      "4700 [D loss: 6.811662] [G loss: 0.000026] [D gradient norm: 0.009131]\n",
      "Time for epoch 4701 is 0.787606954574585 sec\n",
      "4800 [D loss: 44.343559] [G loss: 0.000093] [D gradient norm: 0.000461]\n",
      "Time for epoch 4801 is 0.8567109107971191 sec\n",
      "4900 [D loss: 6.906614] [G loss: 0.000011] [D gradient norm: 0.008482]\n",
      "Time for epoch 4901 is 0.8148596286773682 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-12 11:16:16,246] Trial 11 finished with value: 4016.5606941889614 and parameters: {'num_layers_generator': 6, 'num_layers_discriminator': 6, 'neurons_per_layer_generator': 64, 'neurons_per_layer_discriminator': 256, 'learning_rate': 1.074351501589858e-06, 'n_critic': 4, 'lambda_gp': 6.001204144304466, 'latent_dim': 35, 'optimizer': 'RMSprop'}. Best is trial 11 with value: 4016.5606941889614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 4.844946] [G loss: 0.493808] [D gradient norm: 0.757572]\n",
      "Time for epoch 1 is 4.347909450531006 sec\n",
      "100 [D loss: 4.731856] [G loss: 0.355537] [D gradient norm: 0.674791]\n",
      "Time for epoch 101 is 0.7118070125579834 sec\n",
      "200 [D loss: 4.718980] [G loss: 0.229759] [D gradient norm: 0.533294]\n",
      "Time for epoch 201 is 0.6606955528259277 sec\n",
      "300 [D loss: 4.672799] [G loss: 0.134445] [D gradient norm: 0.391235]\n",
      "Time for epoch 301 is 1.3435606956481934 sec\n",
      "400 [D loss: 4.427530] [G loss: 0.069159] [D gradient norm: 0.242875]\n",
      "Time for epoch 401 is 0.6990177631378174 sec\n",
      "500 [D loss: 4.751837] [G loss: 0.034453] [D gradient norm: 0.139026]\n",
      "Time for epoch 501 is 0.6817755699157715 sec\n",
      "600 [D loss: 4.562175] [G loss: 0.017685] [D gradient norm: 0.081571]\n",
      "Time for epoch 601 is 0.6764528751373291 sec\n",
      "700 [D loss: 4.573939] [G loss: 0.010533] [D gradient norm: 0.053210]\n",
      "Time for epoch 701 is 0.6808159351348877 sec\n",
      "800 [D loss: 4.626626] [G loss: 0.007400] [D gradient norm: 0.040631]\n",
      "Time for epoch 801 is 0.693028450012207 sec\n",
      "900 [D loss: 4.687809] [G loss: 0.005960] [D gradient norm: 0.033382]\n",
      "Time for epoch 901 is 0.7000768184661865 sec\n",
      "1000 [D loss: 4.440746] [G loss: 0.005451] [D gradient norm: 0.030871]\n",
      "Time for epoch 1001 is 0.7612500190734863 sec\n",
      "1100 [D loss: 4.372217] [G loss: 0.005018] [D gradient norm: 0.032136]\n",
      "Time for epoch 1101 is 0.7063882350921631 sec\n",
      "1200 [D loss: 4.804576] [G loss: 0.004833] [D gradient norm: 0.033134]\n",
      "Time for epoch 1201 is 0.7247867584228516 sec\n",
      "1300 [D loss: 4.253202] [G loss: 0.004813] [D gradient norm: 0.031638]\n",
      "Time for epoch 1301 is 0.6756455898284912 sec\n",
      "1400 [D loss: 4.567944] [G loss: 0.004885] [D gradient norm: 0.034549]\n",
      "Time for epoch 1401 is 0.7054650783538818 sec\n",
      "1500 [D loss: 4.598684] [G loss: 0.004816] [D gradient norm: 0.039509]\n",
      "Time for epoch 1501 is 0.7032589912414551 sec\n",
      "1600 [D loss: 4.218998] [G loss: 0.004874] [D gradient norm: 0.045450]\n",
      "Time for epoch 1601 is 0.721174955368042 sec\n",
      "1700 [D loss: 4.297462] [G loss: 0.005220] [D gradient norm: 0.051609]\n",
      "Time for epoch 1701 is 0.4526197910308838 sec\n",
      "1800 [D loss: 4.075713] [G loss: 0.005142] [D gradient norm: 0.068591]\n",
      "Time for epoch 1801 is 0.522634744644165 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-12 11:37:15,383] Trial 12 failed with parameters: {'num_layers_generator': 6, 'num_layers_discriminator': 3, 'neurons_per_layer_generator': 64, 'neurons_per_layer_discriminator': 256, 'learning_rate': 1.1521705846514664e-06, 'n_critic': 4, 'lambda_gp': 4.6143783643002045, 'latent_dim': 48, 'optimizer': 'RMSprop'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Apps\\anaconda3\\envs\\krzysiu\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\KrzysztofJedraszek\\AppData\\Local\\Temp\\ipykernel_15052\\2012784466.py\", line 29, in objective\n",
      "    train(generator, discriminator, combined, data, latent_dim, epochs=5000, batch_size=batch_size, n_critic=n_critic, lambda_gp=lambda_gp, save_interval=100)\n",
      "  File \"C:\\Users\\KrzysztofJedraszek\\AppData\\Local\\Temp\\ipykernel_15052\\24153529.py\", line 132, in train\n",
      "    d_loss_real = discriminator.train_on_batch(real_data, -np.ones((batch_size, 1), dtype=np.float32))\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-12 11:37:15,386] Trial 12 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Repositories\\rna-generator\\optuna.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trial \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\n",
      "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\krzysiu\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\krzysiu\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\krzysiu\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\krzysiu\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\krzysiu\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32md:\\Repositories\\rna-generator\\optuna.ipynb Cell 8\u001b[0m line \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m combined\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mwasserstein_loss, optimizer\u001b[39m=\u001b[39mopt)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Trenowanie modelu\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train(generator, discriminator, combined, data, latent_dim, epochs\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, n_critic\u001b[39m=\u001b[39;49mn_critic, lambda_gp\u001b[39m=\u001b[39;49mlambda_gp, save_interval\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m synthetic_data \u001b[39m=\u001b[39m generate_data(generator, n_samples\u001b[39m=\u001b[39m\u001b[39m57736\u001b[39m, latent_dim\u001b[39m=\u001b[39mlatent_dim, original_min\u001b[39m=\u001b[39moriginal_min, original_max\u001b[39m=\u001b[39moriginal_max)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(synthetic_data)\n",
      "\u001b[1;32md:\\Repositories\\rna-generator\\optuna.ipynb Cell 8\u001b[0m line \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, combined, data, latent_dim, epochs, batch_size, n_critic, lambda_gp, save_interval)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m generated_data \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39mpredict(noise)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# Trenowanie dyskryminatora\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m d_loss_real \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39;49mtrain_on_batch(real_data, \u001b[39m-\u001b[39;49mnp\u001b[39m.\u001b[39;49mones((batch_size, \u001b[39m1\u001b[39;49m), dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m d_loss_fake \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39mtrain_on_batch(generated_data, np\u001b[39m.\u001b[39mones((batch_size, \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Repositories/rna-generator/optuna.ipynb#X10sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m d_loss \u001b[39m=\u001b[39m d_loss_fake \u001b[39m-\u001b[39m d_loss_real \u001b[39m+\u001b[39m lambda_gp \u001b[39m*\u001b[39m gradient_penalty(real_data, generated_data, discriminator)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(' Value: ', trial.value)\n",
    "print(' Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krzysiu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
